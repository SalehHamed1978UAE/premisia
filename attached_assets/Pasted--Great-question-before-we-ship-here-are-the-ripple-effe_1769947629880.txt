 Great question—before we ship, here are the ripple effects to think through so
  we aren’t surprised later:

  ———

  ### 1. Cost & Latency

  - Extra LLM calls: Every workstream (or deliverable) now triggers a request.
    Large programs (20+ workstreams) mean 20+ requests per run. We should budget
    tokens or consider batching.
  - Timeouts: EPM synthesis is already long-running; more sequential LLM calls
    add latency. We may need to run them in parallel with timeouts and
    fallbacks.

  ### 2. Determinism & Reproducibility

  - Non-deterministic outputs: Role titles may vary between runs (“POS Launch
    Lead” vs “POS Implementation Manager”), which could confuse users comparing
    versions.
      - Mitigation: store the generated role with the workstream so reruns reuse
        it unless context changes.
  - Auditability: We should log the prompt + response (or at least the
    rationale) so we can explain why an owner was chosen.

  ### 3. Resource Plan Integrity

  - Explosion of unique roles: If every workstream invents a new title, the
    resource plan could end up with dozens of unique roles, many of which aren’t
    actually staffed. We need a deduping/canonicalization step or a constraint
    (e.g., limit to N unique roles, reuse close matches).
  - Assignments pipeline: Downstream modules expect a finite set of internal
    resources. If the new role isn’t staffed, assignments generation might fail
    unless we automatically add the role to the resource plan (with default
    FTE). We must ensure resource budgets still make sense.

  ### 4. User Experience / UI

  - Naming conventions: LLM might produce long or unusual titles. We may need a
    post-processing step to enforce concise naming (e.g., max 35 chars, Title
    Case).
  - Consistency with templates: If other parts of the UI (like analytics
    dashboards) expect known role categories, we may need mapping back to
    categories (Operations, Marketing, etc.) for grouping.

  ### 5. Safety & Data Privacy

  - Prompt leakage: We’re sending business context to the LLM. Make sure no
    sensitive data (like financial amounts or PII) is included unintentionally.
  - Compliance with AI provider policy: Ensure the prompt doesn’t violate usage
    constraints (e.g., includes publicly shareable data only).

  ### 6. Fallback behavior

  - LLM failure: Network issues or rate limits need a graceful fallback (e.g.,
    revert to template role, log the failure). Otherwise, whole generation could
    crash.
  - Caching: If the same workstream name shows up in multiple runs (e.g.,
    “Regulatory Compliance & Licensing”), we could cache the role title to avoid
    repeated calls and ensure consistency.

  ### 7. Impact on downline analytics

  - Reporting / metrics: Any analytics that aggregate by role (e.g., “How many
    Compliance tasks per program?”) will now see dynamic role names. We may need
    to maintain a mapping to canonical categories for metrics.

  ———

  Recommendation:

  1. Start with LLM inference constrained by a canonical role taxonomy:
      - Use CF to determine the “category” (construction, compliance, etc.).
      - LLM proposes a role name but also returns the category.
      - We use the category for analytics, and we dedupe role names within that
        category.
  2. Implement caching + fallback right away:
      - Cache by businessType + workstreamName.
      - If LLM call fails, fall back to template for that category.
  3. Log outputs & rationales:
      - Helpful for debugging and audit.

  With those safeguards, the approach you outlined works without spiraling into
  surprises. Let me know if you want me to draft the caching/fallback strategy
  or wire up the category mapping.


› lets leave CF for nexr phase. do your plan around that


• Got it—we’ll defer Context Foundry integration to the next phase. For this
  iteration we’ll implement the LLM-based role inference inside the EPM pipeline
  itself. Here’s the concrete plan I’ll execute now:

  1. Add RoleInference helper
      - Build the prompt described earlier and call our existing LLM provider to
        generate { role_title, rationale, confidence }.
      - Cache results per businessType + workstreamName inside the run to avoid
        duplicate calls and keep behavior consistent.
  2. Inject roles into workstreams
      - After workstreams are generated, loop through them.
      - Call the role inference helper.
      - Set ws.owner = roleTitle.
      - Store rationale in ws.metadata.ownerRationale for audits.
  3. Keep resource plan consistent
      - If the inferred role isn’t already in resourcePlan.internalTeam, append
        a stub entry (default FTE, type 'internal').
      - This keeps assignments and exports aware of the new role.
      - Use a simple dedupe (case-insensitive) so we don’t add near-duplicates.
  4. Fallback & logging
      - If the LLM call fails or returns empty, fall back to the existing
        template-based owner (so we don’t break synthesis).
      - Log each generated role + rationale for debugging.
  5. Testing
      - Run a café journey and confirm the Gantt shows construction, POS, HR,
        marketing, compliance roles as expected.
      - Run a non-café journey to ensure the logic adapts.
      - Verify resource plan and assignments include the new roles.

  This gives us the dynamic owner generation you asked for while keeping
  everything self-contained (no CF dependencies yet). Let me know if you want me
  to push this into a branch or just write up the patch for Replit to apply.

